import math
import torch
from typing import Optional
import fvcore.nn.weight_init as weight_init
import torch.nn.functional as F
from torch import nn
from detectron2.layers import Conv2d, ShapeSpec, get_norm
from detectron2.modeling import Backbone, BACKBONE_REGISTRY
from .fpn import LastLevelP6, LastLevelP6P7, LastLevelMaxPool, build_custom_resnet_backbone
import os

class SumCell(nn.Module):
    def __init__(self, in_channels, out_channels, norm_cfg=None, with_out_conv=True):
        super(SumCell, self).__init__()
        dict_convs = dict(groups=1, kernel_size=3, padding=1, bias=True, norm=norm_cfg)
        self.with_out_conv = with_out_conv
        ## relu => conv > norm
        if with_out_conv:
            self.out_conv = Conv2d(
                in_channels, out_channels, **dict_convs)
        self.upsample_mode = 'nearest'
        self.activation = F.relu

    def _resize(self, x, size):
        if x.shape[-2:] == size:
            return x
        elif x.shape[-2:] < size:
            return F.interpolate(x, size=size, mode=self.upsample_mode)
        else:
            if x.shape[-2] % size[-2] != 0 or x.shape[-1] % size[-1] != 0:
                h, w = x.shape[-2:]
                target_h, target_w = size
                pad_h = math.ceil(h / target_h) * target_h - h
                pad_w = math.ceil(w / target_w) * target_w - w
                pad_l = pad_w // 2
                pad_r = pad_w - pad_l
                pad_t = pad_h // 2
                pad_b = pad_h - pad_t
                pad = (pad_l, pad_r, pad_t, pad_b)
                x = F.pad(x, pad, mode='constant', value=0.0)
            kernel_size = (x.shape[-2] // size[-2], x.shape[-1] // size[-1])
            x = F.max_pool2d(x, kernel_size=kernel_size, stride=kernel_size)
            return x

    def _binary_op(self, x1, x2):
        return x1 + x2
    def forward(self, x1, x2, out_size):
        if out_size is None:  # resize to larger one
            out_size = max(x1.size()[2:], x2.size()[2:])
        x1 = self._resize(x1, out_size)
        x2 = self._resize(x2, out_size)
        x = self._binary_op(x1, x2)
        if self.with_out_conv:
            x = self.activation(x)
            x = self.out_conv(x)
        return x

class GlobalPoolingCell(SumCell):

    def __init__(self,
                 in_channels: Optional[int] = None,
                 out_channels: Optional[int] = None,
                 **kwargs):
        super().__init__(in_channels, out_channels, **kwargs)
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
    def _binary_op(self, x1, x2):
        x2_att = self.global_pool(x2).sigmoid()
        return x2 + x2_att * x1

def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert stride == 2 * strides[i - 1], "Strides {} {} are not log2 contiguous".format(
            stride, strides[i - 1]
        )

class NASFPN(Backbone):
    """
    This module implements :paper:`FPN`.
    It creates pyramid features built on top of some input feature maps.
    """

    def __init__(
        self, bottom_up, backbone_type, in_features, out_channels,
            norm="", top_block=None, fuse_type="sum", stack_times=7,
    ):
        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            out_channels (int): number of channels in the output feature maps.
            norm (str): the normalization to use.
            top_block (nn.Module or None): if provided, an extra operation will
                be performed on the output of the last (smallest resolution)
                FPN output, and the result will extend the result list. The top_block
                further downsamples the feature map. It must have an attribute
                "num_levels", meaning the number of extra FPN levels added by
                this block, and "in_feature", which is a string representing
                its input feature (e.g., p5).
            fuse_type (str): types for fusing the top down features and the lateral
                ones. It can be "sum" (default), which sums up element-wise; or "avg",
                which takes the element-wise mean of the two.
        """
        super(NASFPN, self).__init__()
        # Feature map strides and channels from the bottom up network (e.g. ResNet)
        strides = [4, 8, 16, 32]
        norm = 'BN'
        assert backbone_type in ["res", "res18", "eff_b2", "eff_b0",
                                 "convnext", "inception", "mobilenet_v2",
                                 "vgg16"]
        if backbone_type == "res":
            in_channels_per_feature = [256, 512, 1024, 2048]
            self.func_bottom_up = self.extract_bottom_res
        elif backbone_type == "res18":
            in_channels_per_feature = [64, 128, 256, 512]
            self.func_bottom_up = self.extract_bottom_res
        elif backbone_type == "eff_b2":
            in_channels_per_feature = [24, 48, 120, 352]
            self.func_bottom_up = self.extract_bottom_eff
        elif backbone_type == "eff_b0":
            in_channels_per_feature = [24, 40, 112, 320]
            self.func_bottom_up = self.extract_bottom_eff
        elif backbone_type == "mobilenet_v2":
            in_channels_per_feature = [24, 32, 96, 320]
            self.func_bottom_up = self.extract_bottom_eff
        elif backbone_type == 'convnext':
            in_channels_per_feature = [128, 256, 512, 1024]
            self.func_bottom_up = self.extract_bottom_convnext

        _assert_strides_are_log2_contiguous(strides)
        lateral_convs = []
        use_bias = norm == ""
        for idx, in_channels in enumerate(in_channels_per_feature):
            lateral_norm = get_norm(norm, out_channels)
            lateral_conv = Conv2d(
                in_channels, out_channels, kernel_size=1, bias=use_bias, norm=lateral_norm
            )
            weight_init.c2_xavier_fill(lateral_conv)
            stage = int(math.log2(strides[idx]))
            self.add_module("fpn_lateral{}".format(stage), lateral_conv)
            lateral_convs.append(lateral_conv)
        # Place convs into top-down order (from low to high resolution)
        # to make the top-down computation in forward clearer.
        self.lateral_convs = lateral_convs#[::-1]
        self.top_block = top_block
        self.in_features = tuple(in_features)
        self.bottom_up = bottom_up
        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {"p{}".format(int(math.log2(s))): s for s in strides}
        # top block output feature maps.
        if self.top_block is not None:
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1)

        self._out_features = list(self._out_feature_strides.keys())
        self._out_feature_channels = {k: out_channels for k in self._out_features}
        self._size_divisibility = strides[-1]
        assert fuse_type in {"avg", "sum"}
        self._fuse_type = fuse_type
        extra_conv = Conv2d(
            out_channels, out_channels, 1, norm=get_norm(norm, out_channels))
        self.extra_downsample = nn.Sequential(extra_conv, nn.MaxPool2d(2, 2))
        self.stack_times = stack_times
        # add NAS FPN connections
        self.fpn_stages = nn.ModuleList()

        for _ in range(self.stack_times):
            stage = nn.ModuleDict()
            # gp(p6, p4) -> p4_1
            stage['gp_64_4'] = GlobalPoolingCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm ,out_channels))
            # sum(p4_1, p4) -> p4_2
            stage['sum_44_4'] = SumCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm ,out_channels))
            # sum(p4_2, p3) -> p3_out
            stage['sum_43_3'] = SumCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm, out_channels))
            # sum(p3_out, p4_2) -> p4_out
            stage['sum_34_4'] = SumCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm, out_channels))
            # sum(p5, gp(p4_out, p3_out)) -> p5_out
            stage['gp_43_5'] = GlobalPoolingCell(with_out_conv=False)
            stage['sum_55_5'] = SumCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm, out_channels))
            # sum(p7, gp(p5_out, p4_2)) -> p7_out
            stage['gp_54_7'] = GlobalPoolingCell(with_out_conv=False)
            stage['sum_77_7'] = SumCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm, out_channels))
            # gp(p7_out, p5_out) -> p6_out
            stage['gp_75_6'] = GlobalPoolingCell(
                in_channels=out_channels,
                out_channels=out_channels,
                norm_cfg=get_norm(norm, out_channels))
            self.fpn_stages.append(stage)

    @property
    def size_divisibility(self):
        return self._size_divisibility

    def extract_bottom_res(self, x):
        return self.bottom_up(x)
    def extract_bottom_vgg(self, x):
        feats = self.bottom_up(x)[2:]
        return {k: feat for k, feat in zip(self.in_features, feats)}
    def extract_bottom_eff(self, x):
        feats = self.bottom_up(x)[1:]
        return {k: feat for k, feat in zip(self.in_features, feats)}
    def extract_bottom_convnext(self, x):
        feats = self.bottom_up(x)
        return {k: feat for k, feat in zip(self.in_features, feats)}

    def forward(self, x, stop_grad=False):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
                feature map tensor for each feature level in high to low resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to FPN feature map tensor
                in high to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p6"].
        """
        bottom_up_features = self.func_bottom_up(x)
        if stop_grad:
            bottom_up_features = {key: value.detach() for key, value in bottom_up_features.items()}
        results = []

        feats = [
            lateral_conv(bottom_up_features[self.in_features[i]])
            for i, lateral_conv in enumerate(self.lateral_convs)]
        #for downsample in self.extra_downsamples:
        feats.append(self.extra_downsample(feats[-1]))
        p3, p4, p5, p6, p7 = feats
        for stage in self.fpn_stages:
            # gp(p6, p4) -> p4_1
            p4_1 = stage['gp_64_4'](p6, p4, out_size=p4.shape[-2:])
            # sum(p4_1, p4) -> p4_2
            p4_2 = stage['sum_44_4'](p4_1, p4, out_size=p4.shape[-2:])
            # sum(p4_2, p3) -> p3_out
            p3 = stage['sum_43_3'](p4_2, p3, out_size=p3.shape[-2:])
            # sum(p3_out, p4_2) -> p4_out
            p4 = stage['sum_34_4'](p3, p4_2, out_size=p4.shape[-2:])
            # sum(p5, gp(p4_out, p3_out)) -> p5_out
            p5_tmp = stage['gp_43_5'](p4, p3, out_size=p5.shape[-2:])
            p5 = stage['sum_55_5'](p5, p5_tmp, out_size=p5.shape[-2:])
            # sum(p7, gp(p5_out, p4_2)) -> p7_out
            p7_tmp = stage['gp_54_7'](p5, p4_2, out_size=p7.shape[-2:])
            p7 = stage['sum_77_7'](p7, p7_tmp, out_size=p7.shape[-2:])
            # gp(p7_out, p5_out) -> p6_out
            p6 = stage['gp_75_6'](p7, p5, out_size=p6.shape[-2:])
        #assert len(self._out_features) == len(results)
        results = [p3, p4, p5, p6, p7]
        return {f: res for f, res in zip(self._out_features, results)}

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }

@BACKBONE_REGISTRY.register()
def build_custom_resnet_nasfpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a detectron2 CfgNode

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_custom_resnet_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    out_channels = cfg.MODEL.FPN.OUT_CHANNELS
    backbone = NASFPN(
        bottom_up=bottom_up,
        backbone_type='res',
        in_features=in_features,
        out_channels=out_channels,
        norm=cfg.MODEL.FPN.NORM,
        top_block=LastLevelMaxPool(),
        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,
    )
    return backbone
